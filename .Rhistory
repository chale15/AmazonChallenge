qnorm(.95) #= 1.644854
qnorm(.05) #=-1.644854
#to find the sample average:
mean(prostate$age) #= 63.86598
#to find the n
#row(prostate) #n = 97
#to sd
sd(prostate$age) # = 7.445117
#To find margin of error: me = qnorm(.95)*(sd/sqrt(n))
1.644*(7.445117/sqrt(97)) #= 1.242761
#to find the lower and upper bounds:
#mean(x) - me, mean(x) + me
mean(prostate$age) - 1.242761
mean(prostate$age) + 1.242761
#We are 90% confident that the [population parameter] is between 62.62322, 65.10874 or 62.62, 65.11.
##95% CI:
#95% CI means alpha = .05 We can get z(alpha/2) = z(.025) from R: > qnorm(.975)
qnorm(.975) #= -1.959964
qnorm(.025) #= -1.959964
#to find the sample average:
mean(prostate$age) #= 63.86598
#to find the n:
row(prostate) # n = 97
#to sd
sd(prostate$age) # = 7.445117
#To find margin of error: me = qnorm(.975)*(sd/sqrt(n))
1.959964 * (7.445117/sqrt(97)) #= 1.48161
#to find the lower and upper bounds:
#mean(x) - me, mean(x) + me
#mean(prostate$age) - 1.48161 = 63.86598 - 1.48161 = 62.38437
#mean(prostate$age) + 1.48161 = 63.86598 + 1.48161 = 65.34759
mean(prostate$age) - 1.48161
mean(prostate$age) + 1.48161
#We are 95% confident that the [population parameter] is between 62.38437, 65.34759 (or 62.38, 65.35).
confint(prostate1, level = 0.95)
confint(prostate1, level = 0.90)
#Answer: At 90% for the independent variable of age, we find 0 outside the boundaries. Age is thus statistically significant and well estimated, meaning age and lpsa have higher association. If 0 is outside, the p-value is lower. Refers to a p-value of 0.1. At 95% for the independent variable of age, we find the 0 between the max and min boundaries (the confidence interval). Age is thus not statistically significant, not well estimated because we don't know whether the effect of the variable on the outcome is pos or neg, so we don't know directionality; we thus don't know whether the variables are associated neg or pos. Thus, the input and output have low statistical association. Because the confidence interval includes 0, the coefficient estimate or variable age is NOT statistically significant at the point .05 level. Refers to a p-value of 0.05. We know the p-value is larger than .05, this is why it's not significant.
data(prostate) #with all 8 predictors in
#my model:
prostate1 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
summary(prostate1)
#Code to compare sample variances:
#Divide the variance of the y by each x, using var(), to get the co-variances between the two variables:
var(prostate$lpsa, prostate$lcavol, na.rm = TRUE)
var(prostate$lpsa, prostate$lweight, na.rm = TRUE)
var(prostate$lpsa, prostate$age, na.rm = TRUE)
var(prostate$lpsa, prostate$lcp, na.rm = TRUE)
var(prostate$lpsa, prostate$svi, na.rm = TRUE)
var(prostate$lpsa, prostate$gleason, na.rm = TRUE)
var(prostate$lpsa, prostate$pgg45, na.rm = TRUE)
var(prostate$lpsa, prostate$lbph, na.rm = TRUE)
anova(prostate1)
#Conclusion: Under the DF column, the degrees of freedom are always 1, so adding more predictor variables does not give me more degrees of freedom.
#One code to do a permutation test:
#install.packages("coin")
library(coin)
#EXAMPLE:
#data(iris)
#df <- subset(iris, select = c(Sepal.Length, Species))
#3 Types of code which should do the same thing, a permutation f-test test:
#test <- oneway_test(Sepal.Length ~ Species, data = df, type = "F_test") #Check
#test
#Predictors are significantly different than 0 because we have a small p-value.
#Second code:
#test2 <- var.test(Sepal.Length ~ Species, data = df, type = "F_test")
#test2
#Third code to do a permutation test -- code works but be patient because output returns slowly and gets burried:
#install.packages("onewaytests")
#library(onewaytests)
#code
#pf.test(formula, data, N = ?, alpha = 0.05, na.rm = TRUE, verbose = TRUE)
#?2 <- pf.test(response ~ predictor data = ?)
#paircomp(?2)
#Example:
#pf.test(Sepal.Length ~ Species, data = iris)
#out <- pf.test(Sepal.Length ~ Species, data = iris)
#Starting here is code for different f-tests but I only need to do one if asked:
#paircomp(model) or
#paircomp(data, labels = NULL, mscale = NULL, ordered = FALSE, covariates = NULL)
#paircomp(x, adjust.method = c("bonferroni", "holm", "hochberg", "hommel", "BH",
#  "BY", "fdr", "none"), verbose = TRUE, ...)
#Tells you to reject the null if the p-value is 0 or close to it.
# Johansen F test (alpha = 0.05): (Output returns slowly. Be patient.)
#onewaytests(Sepal.Length ~ Species, data = iris, method = "johansen")
#One-Way Analysis of Variance (alpha = 0.05):
#ANOVA is the regular f-test v permutation test as an f-test:
#out2 <- onewaytests(Sepal.Length ~ Species, data = iris, method = "aov")
#Steps to get confidence intervals for 99% Significance Level:
#99% CI means alpha = .025 We can get z(alpha/2) = z(.0125) from R: > qnorm(.9875)
qnorm(.9875) #= 2.241403
qnorm(.0125) #= -2.241403
#to find the sample average:
mean(prostate$age) #= 63.86598
#to find the n:
row(prostate) # n = 97
#to sd
sd(prostate$age) # = 7.445117
#To find margin of error: me = qnorm(.9875)*(sd/sqrt(n))
2.241403 * (7.445117/sqrt(97)) #= 1.69436
#coefficient ("point estimate") for the predictor for age: -0.019637
#Margin of Error: 1.69436
#to find the lower and upper bounds: mean(x) - me, mean(x) + me
#coefficient ("point estimate") for the predictor - Margin of Error =
#coefficient ("point estimate") for the predictor + Margin of Error =
-0.019637 - 1.69436 #-1.713997, CI at 99% significance level
-0.019637 + 1.69436 #1.674723, CI at 99% significance level
#CI: Does the CI include 0? Yes. Conclusion: CIs: -0.03927913, 0.00000513. Does the CI include 0? Yes. Because the confidence interval for age as a predictor of the protein contains 0, it is possible that the slope is 0. As a result, we're not confident the slope is different than 0 because it could just be 0. Because this ranges below and above 0, it includes 0, and thus for age (a potential predictor of the protein) we cannot reject the null hypothesis. We thus do not have enough statistically significant evidence that there is a linear relationship between age and the protein. As a result, we could have predicted the p-value to be above .05.
#One-Way Analysis of Variance (alpha = 0.05):
#ANOVAprostate <- oneway_test(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate, method = "aov")
#paircomp(ANOVAprostate)
#ANOVAprostate    #not works
#out5 <- onewaytests(prostate$lpsa ~ prostate$lcavol + prostate$lweight + prostate$age + prostate$lbph + prostate$svi + prostate$lcp + prostate$gleason + prostate$pgg45, method = "aov")
#paircomp(out5)
#out5             #not works
#Other Method to do the Permutation Test:
nreps <- 5000 # 5000 permutation(re-arrangements of data) (re-arranwe will test. how many reps.
set.seed(123) # Always set the seed to make it replicable.
fstats <- numeric(nreps) # Define an empty  vector of 5000 obs, and for every repetition I put f-stat into that vector. This creates a placeholder to put my answsers in. Output is a list of f-stats. A place  tocreate the object in which we will save results.
for(i in 1:nreps){ # loop from 1 to 5000, the total number of permutations we will test. i = "each". Do this code 5000 times, or do the forloop 5000 times.  "for(i in 1:nreps)" = do what comes after the curly brackets 5000 times bc nreps = 5000.
#Create a forloop:
#(prostate$lpsa) = sample from the dep variable every time, why dep var has "sample" wrapped around it
temp <- lm(sample(prostate$lpsa) ~ prostate$lcavol + prostate$lweight +
prostate$age + prostate$lbph + prostate$svi + prostate$lcp +
prostate$gleason + prostate$pgg45) #sample = randomly re-sampling from my outcome, then by creating the "temp" model, I'm creating a model with all of these outcomes. #Run the regression, Call it "temp" bc everytime I run the regression I redefine it, every sample is different. The code is randomizing the order of the samples. get a new the sample() function generates random permutations (1 by default) #The temp model gets re-written 5000 times inside the forloop.
fstats[i] <- summary(temp)$fstat[1] #[i] = "For each lot in fstats, put the following thing..." # "summary(temp)$fstat" = pulls the fstat out of each temp model I'm going to create. #[1] gives me just the first stat in the fstat and not the extra like degrees of freedoms, so R stores just the first stat/number in the fstats vector that I created. #This stores my f-stat and stores every f-state in the first and then the second and then the third object of the vector... O produce a vector that is a list of f-stats and see if and indiv obs in the output of the output of the forloop is bigger than the f-stats when run on my real data. i = index (This is my 250th run for example.) Pulls out the f-stat for every regression and sticking it in 250th spot of the blank vector. This helps me avoid overwriting the results I already ran. save the F-statistic value. #Fstat is a stat by itself, a global test that checks whether one of my co-efficients is non-zero, v summary gives me a bunch of stats.
}
#Answer: not any. 0 of the 5000 f-statistics of random pairings are larger than mine.
#create a new, shorter name for the summary of my original model (the one without permutations or no permuted info):
sumorigdata <- summary(prostate1)
# [1] = take the first number of the line = 20.86135
mean(fstats > sumorigdata$fstatistic[1]) # This will tell me what share of the  random f-stats are bigger than the f-stats than running the model on the real data (original model). if many are--> my data is random and has no systematic rela bt inputs and outputs. What proportion of the permuted F-stats are greater than the observed?
# there is a small but noticable difference between this p-value and the one we got based on normal theory
#Answer: Either the mean(fstats) is the same or not bigger. Because my F-statistic is 51.99 on 3 and 93 DF and this F-stat is 0, the results obtained from our model are very unlikley to be due to random chance.
#ADJUSTED R-SQUARE:
#original linear model with 8 predictors:
prostate1 <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate)
summary(prostate1)
#linear model with 3 predictors, those significant at the %5 level:
prostate3 <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
summary(prostate3)
#In the original or full model with 8 predictors, three predictors are significant at the 5% level and the Adjusted R-squared is 0.6234. In the model with 3 predictors significant at the 5% level, the Adjusted R-squared is 0.6144, slightly lower than the full model.
#CORRELATION:
#original model's correlation:
cor(prostate$lpsa, prostate$lcavol + prostate$lweight + prostate$age + prostate$lbph + prostate$lcp + prostate$gleason + prostate$pgg45)
#F-STAT TEST:
#1.Frame the null H0: hypothesizing/assuming the variances are equal. HA: variances are not equal.
#Compare the F tests of the restricted and unrestricted models and the difference in the F stats will be a chi-square test statistic with degrees of freedom equal to the number of fewer variables in the smaller model compared to the larger:
#ANOVA:
anova(prostate3, prostate1)
#The p-value is larger than our pre-set .05 level; we thus fail to reject the full model.
#OTHER WAYS TO DO THIS:
#ANOVA is the regular f-test v permutation test as an f-test:
#ANOVAsmallmodel <- oneway_test(prostate$lpsa, prostate$lcavol + prostate$lweight + prostate$svi, method = "aov") #code doesn't work so I cannot compare models
#F-STAT PERMUTATION TEST:
#The f-test tests the whole model and all my data v the permutation f-test tests different samples of my data.
#prostate4 <- var.test(prostate$lpsa ~ prostate$lcavol + #prostate$lweight + prostate$svi, type = "F_test")
#prostate4
0.669337 + ((0.587022*1.44692) + (0.454467*3.62301) + (-0.019637*65) + (0.107054*0.30010) + (0.766157*0) + (-0.105474*-0.79851) + (0.045142*7) + (0.004525*15))
mx <- (0.587022*1.44692) + (0.454467*3.62301) + (-0.019637*65) + (0.107054*0.30010) + (0.766157*0) + (-0.105474*-0.79851) + (0.045142*7) + (0.004525*15)
b <- 0.669337
mx + b
#predict at appropriate 95% prediction interval:
x=data.frame(lcavol = 1.44692, lweight = 3.62301, age = 65.00000, lbph=0.30010, svi = 0.00000, lcp = -0.79851, gleason = 7.00000, pgg45 = 15.00000)
predict(prostate1, x, interval="prediction")
#0.9646584 lower, 3.813447 upper, my prediction intervals
#95% CI means alpha = .05 We can get z(alpha/2) = z(.025) from R: > qnorm(.975)
qnorm(.975) #= 1.959964
qnorm(.025) #= -1.959964
#to find the sample average, it's mean(prostate$age), but this time I have real age (65) #Do I use the actual age 65?
#to find the n:
row(prostate) # n = 97
#to sd
sd(prostate$age) # = 7.445117
#To find margin of error: me = qnorm(.975)*(sd/sqrt(n))
1.959964 * (7.445117/sqrt(97)) #= 1.48161
#to find the lower and upper bounds: mean(x) - me, mean(x) + me
65 - 1.48161 #63.51839
65 + 1.48161 #66.48161
#conclusion: We predict that the lpsa for this patient is 2.389062 and we are 95% confident that the predicted lpsa for this patient falls between the interval 0.9646584 lower, 3.813447.
0.669337 + ((0.587022*1.44692) + (0.454467*3.62301) + (-0.019637*20) + (0.107054*0.30010) + (0.766157*0) + (-0.105474*-0.79851) + (0.045142*7) + (0.004525*15))
mx <- (0.587022*1.44692) + (0.454467*3.62301) + (-0.019637*20) + (0.107054*0.30010) + (0.766157*0) + (-0.105474*-0.79851) + (0.045142*7) + (0.004525*15)
b <- 0.669337
mx + b
#At 65, the patient has a predicted lpsa at 2.389062 but at 20 years old the same patient has a predicted lpsa at 3.272727.
#predict at appropriate 95% prediction interval:
x2=data.frame(lcavol = 1.44692, lweight = 3.62301, age = 20.00000, lbph=0.30010, svi = 0.00000, lcp = -0.79851, gleason = 7.00000, pgg45 = 15.00000)
predict(prostate1, x2, interval="prediction")
#1.538744 lower, 5.006707 upper
#95% CI means alpha = .05 We can get z(alpha/2) = z(.025) from R: > qnorm(.975)
qnorm(.975) #= 1.959964
qnorm(.025) #= -1.959964
#to find the sample average, it's mean(prostate$age), but this time I have real age (20)
#to find the n:
row(prostate) # n = 97
#to sd
sd(prostate$age) # = 7.445117
#To find margin of error: me = qnorm(.975)*(sd/sqrt(n))
1.959964 * (7.445117/sqrt(97)) #= 1.48161
#to find the lower and upper bounds: mean(x) - me, mean(x) + me
20 - 1.48161 #18.51839
20 + 1.48161 #21.48161
#We predict that the lpsa for this patient is 3.272727 and we are 95% confident that the predicted lpsa for this patient falls between the interval 1.538744 lower, 5.006707 upper.
3.813447-0.9646584 #2.848789
5.006707-1.538744 #3.467963
#linear model with 3 predictors, those significant at the %5 level:
prostate3 <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
summary(prostate3)
0.669337 + ((0.587022*1.44692) + (0.454467*3.62301) + (0.766157*0))
mx1 <- ((0.587022*1.44692) + (0.454467*3.62301) + (0.766157*0))
b1 <- -0.26809
mx1 + b1 #2.227822 = predicted y with smaller model
#predict at appropriate 95% prediction interval:
xsmallermodel=data.frame(lcavol = 1.44692, lweight = 3.62301, svi = 0.00000)
predict(prostate3, xsmallermodel, interval="prediction")
#0.9383436 lower, 3.806724 upper
#comparing 3 intervals:
3.806724 - 0.9383436 #average patient is 2.86838
5.006707 - 1.538744 #patient at 20 yrs old is 3.467963
3.813447 - 0.964658 #patient at 65 yrs old is 2.848789
#CONCLUSION: We predict that under the smaller model with only 3 predictors that are significant at the 95% level, the predicted lpsa for patients is 2.227822, and we are 95% confident that the predicted lpsa for this patient falls between the interval 0.9383436 lower, 3.806724 upper. The above interval is smaller than the interval (1.538744, 5.006707) for a patient at 20 years old, but larger than the interval for the patient at 65 years old (0.9646584, 3.813447). Thus, we have more certainty about cancer in older people. (Having a true age helps us decrease the size of the confidence interval. A smaller CI is better because because I have more certainty what the true population parameter is.)
#CODE:
# in case we want to use a seed
set.seed(5)
## 70% of the sample size = approx. 68 men
train_size <- floor(0.70 * nrow(prostate))
in_rows <- sample(c(1:nrow(prostate)), size = train_size, replace = FALSE)
train <- prostate[in_rows, ]
test <- prostate[-in_rows, ]
train
test
trainsetfullmod <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = train)
summary(trainsetfullmod)
#As opposed to the full model where lcavol, lweight, and svi are significant at the 95% level, here in the training set (70% of the data), only lcavol and age (an unexpected predictor) are significant at the 95% level. The Adjusted R-squared is 0.6676.
#linear model with 3 predictors, those significant at the %5 level:
trainrestrictedmod <- lm(lpsa ~ lcavol + lweight + svi, data = train)
summary(trainrestrictedmod)
#As opposed to the restricted model where lcavol, lweight, and svi are significant at the 95% level, here in the restricted training set, only lcavol and weight are significant at the 95% level but svi is not. The Adjusted R-squared is 0.6418.
testsetfullmod <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = test)
summary(testsetfullmod)
#As opposed to the full model where lcavol, lweight, and svi are significant at the 95% level, or the full model training set where lcavol and age are significant at the 95% level, in this full model test set lcavol and svi are significant at the 95% level. The Adjusted R-squared is lower at 0.5753. Conclusion: Although the Adjusted R-squared of the full model test set is lower at 0.5753, the full model test set, only 30% of the data, better predicted significant predictors than the full model training set where 70% of the data was available.
#linear model with 3 predictors, those in original full model significant at the %5 level:
testrestrictedmod <- lm(lpsa ~ lcavol + lweight + svi, data = test)
summary(testrestrictedmod)
#Like the original model and original restricted model, where lcavol, lweight, and svi are significant at the 95% level, here in the restricted test set lcavol, lweight, and svi are all again significant at the 95% level. Also, the Adjusted R-squared for the restricted test set model is 0.612.
set.seed(388)
prostateGLM <- glm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate, family = "gaussian")
set.seed(388)
cv.prostateGLM <- cv.glm(data=prostate, glmfit = prostateGLM, K = 3)
cv.prostateGLM$delta #Here, I add $delta in the cv.glm object to to get the delta values from the models, providing the cross-validated error rates from the model (usually the mean squared error). The cv.prostateGLM$delta gives me two values: The first value is the raw cross-validation estimate of the prediction error (mean squared error). The second value is the bias-corrected estimate, which accounts for the bias in the raw estimate. Using these will evaluate the model’s performance. Including this in my report will allow me to comment on how well my model generalizes to unseen data. https://www.rdocumentation.org/packages/boot/versions/1.3-30/topics/cv.glm
set.seed(388)
loocv.prostateGLM <- cv.glm(data=prostate, glmfit = prostateGLM, K = nrow(prostate))
#loocv.prostateGLM
loocv.prostateGLM$delta
modelSignif.05 <- glm(formula = lpsa ~ lcavol + lweight + svi, data = prostate, family = "gaussian")
cv.modelSignif.05 <- cv.glm(data=prostate, glmfit = modelSignif.05, K = 3)
cv.modelSignif.05$delta
install.packages("corrr")
rm(list=ls())
install.packages("dplyr")
library(dplyr)
install.packages("magrittr")
library(tidyverse)
library(magrittr)
library(ggplot2)
install.packages("texreg")
library(texreg)
library(stargazer)
library(performance)
install.packages("magrittr")
install.packages("dplyr")
#Once I have downloaded the newest version of R, the following ggplots will run:
#R Coding NOTE: ggplot histogram allows for more fine grain breaks than geom-bar. Also, consider using density plot over histogram but it can be preference.
#Cut:
ggplot(diamonds, aes(y = price, x = cut)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(x = color, fill=cut)) +
geom_bar(position = "dodge")+
theme_bw()
diamonds %>%
filter(carat <= 2) %>% #***Does this temporarily filter carat as =2 because I do not want to permanently change the variable?
ggplot(aes(x = carat, fill=cut)) +
geom_bar(position = "dodge") +
theme_bw()
# ***The code does not run.
#Color:
ggplot(diamonds, aes(y = price, x = color)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(x = clarity, fill=color)) +
geom_bar(position = "dodge")+
theme_bw()
#Clarity:
ggplot(diamonds, aes(y = price, x = clarity)) +
geom_jitter() +
theme_bw()
# talk about how this is surprising
#Carat:
ggplot(diamonds, aes(y = price, x = carat)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(y = price, x = carat)) +
geom_point() +
geom_smooth(method = "lm") +
theme_bw()
# ran this based on the strength of the relationship
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
library(tidyverse)
library(ggplot2)
expected_prior <- sum(c(0.1, .3, .6, .8, .95)*0.2)
prior <- rep(0.2, 5)
theta <- c(0.1, .3, .6, .8, .95)
likelihood <- dbinom(21, 68, theta)
posterior <- likelihood * 0.2 / (0.2*sum(likelihood))
print(posterior)
plot(y = prior,
x = theta,
ylim = c(0, 1),
title('Prior and Posterior Probability'),
ylab = 'Probability', xlab = expression(theta),
pch = 19, col = 'cornflowerblue')
points(x = theta, y = posterior, col = 'forestgreen', pch = 17)
legend('topright', legend=c("Prior", "Posterior"), pch=c(19, 17),
col=c("cornflowerblue", "forestgreen"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
beta_prime <- beta + (116 - 17) #Observed Failures
alpha <- 1
beta <- 4
alpha_prime <- alpha + 17 #Observed Successes
beta_prime <- beta + (116 - 17) #Observed Failures
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- alpha_prime*beta_prime / (((alpha_prime + beta_prime)^2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)^2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)*(alpha_prime + beta_prime + 1))
#From Expected Value and Variance for Beta Distribution
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)*(alpha_prime + beta_prime + 1))
cred_int_95 <- qbeta(c(.025, .975), alpha_prime,beta_prime)
#Beta(2,2)
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta(2,2)")
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 20, 20), add=T, lwd=2, col='cornflowerblue')
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 20, 20), add=T, lwd=2, col='cornflowerblue')
curve(dbeta(x, 2, 6), add=T, lwd=2, col='firebrick')
curve(dbeta(x, 6, 2), add=T, lwd=2, col='forestgreen')
alphas <- c(2, 20, 2, 6)
curve(dbeta(x, 1, 1.439))#,
curve(dbeta(x, 1, 1.439))
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5),xlim = c(0,2), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 2), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
1.439*4
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 2), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5,756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5,756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5.756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta Distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5.756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
beta_var_1 <- (2*2.878) / (((2 + 2.878)**2)*(2 + 2.878 + 1))
beta_var_2 <- (4*5.756) / (((4 + 5.756)**2)*(4 + 5.756 + 1))
.04>.02
prob_3f <- qbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
#Beta(2, 6)
curve(dbeta(x, 2, 7), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta(2,6)")
prob_3f <- dbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
prob_3f <- dbeta(0.1, shape1 = 2, shape2 = 7)
prob_3f <- pbeta(0.1, shape1 = 2, shape2 = 7)
prob_3f <- pbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
quantile_3g <- qbeta(0.4, shape1 = 2, shape2 = 7, lower.tail = TRUE)
library(readr)
action_movies <- read_csv("Desktop/Fall 2024/Stat 386/GitHubRepos/lab-06-chale15/action_movies.csv")
View(action_movies)
cor(action_movies$Score, action_movies$Rank)
install.packages('kernlab')
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(naivebayes)
library(ranger)
library(kknn)
library(discrim)
library(kernlab)
#setwd("~/Desktop/Fall 2024/Stat 348/GitHubRepos/amazon-employee-access-challenge/")
setwd("~/Kaggle/AmazonChallenge")
setwd("~/Desktop/Fall 2024/Stat 348/GitHubRepos/amazon-employee-access-challenge/")
train <- vroom("train.csv")
test <- vroom("test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=1)
svm_radial <- svm_rbf(rbf_sigma=tune(),
cost=tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
radial_wf <- workflow() %>%
add_model(svm_radial) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(rbf_sigma(), cost(), levels = 5)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- radial_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
radial_workflow <- workflow() %>%
add_model(svm_radial) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(rbf_sigma(), cost(), levels = 5)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- radial_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- radial_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
radial_svm_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
radial_svm_submission <- radial_svm_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=radial_svm_submission, file="./Submissions/RadialSVMpreds2.csv", delim=",")
