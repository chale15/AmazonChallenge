b1 <- -0.26809
mx1 + b1 #2.227822 = predicted y with smaller model
#predict at appropriate 95% prediction interval:
xsmallermodel=data.frame(lcavol = 1.44692, lweight = 3.62301, svi = 0.00000)
predict(prostate3, xsmallermodel, interval="prediction")
#0.9383436 lower, 3.806724 upper
#comparing 3 intervals:
3.806724 - 0.9383436 #average patient is 2.86838
5.006707 - 1.538744 #patient at 20 yrs old is 3.467963
3.813447 - 0.964658 #patient at 65 yrs old is 2.848789
#CONCLUSION: We predict that under the smaller model with only 3 predictors that are significant at the 95% level, the predicted lpsa for patients is 2.227822, and we are 95% confident that the predicted lpsa for this patient falls between the interval 0.9383436 lower, 3.806724 upper. The above interval is smaller than the interval (1.538744, 5.006707) for a patient at 20 years old, but larger than the interval for the patient at 65 years old (0.9646584, 3.813447). Thus, we have more certainty about cancer in older people. (Having a true age helps us decrease the size of the confidence interval. A smaller CI is better because because I have more certainty what the true population parameter is.)
#CODE:
# in case we want to use a seed
set.seed(5)
## 70% of the sample size = approx. 68 men
train_size <- floor(0.70 * nrow(prostate))
in_rows <- sample(c(1:nrow(prostate)), size = train_size, replace = FALSE)
train <- prostate[in_rows, ]
test <- prostate[-in_rows, ]
train
test
trainsetfullmod <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = train)
summary(trainsetfullmod)
#As opposed to the full model where lcavol, lweight, and svi are significant at the 95% level, here in the training set (70% of the data), only lcavol and age (an unexpected predictor) are significant at the 95% level. The Adjusted R-squared is 0.6676.
#linear model with 3 predictors, those significant at the %5 level:
trainrestrictedmod <- lm(lpsa ~ lcavol + lweight + svi, data = train)
summary(trainrestrictedmod)
#As opposed to the restricted model where lcavol, lweight, and svi are significant at the 95% level, here in the restricted training set, only lcavol and weight are significant at the 95% level but svi is not. The Adjusted R-squared is 0.6418.
testsetfullmod <- lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = test)
summary(testsetfullmod)
#As opposed to the full model where lcavol, lweight, and svi are significant at the 95% level, or the full model training set where lcavol and age are significant at the 95% level, in this full model test set lcavol and svi are significant at the 95% level. The Adjusted R-squared is lower at 0.5753. Conclusion: Although the Adjusted R-squared of the full model test set is lower at 0.5753, the full model test set, only 30% of the data, better predicted significant predictors than the full model training set where 70% of the data was available.
#linear model with 3 predictors, those in original full model significant at the %5 level:
testrestrictedmod <- lm(lpsa ~ lcavol + lweight + svi, data = test)
summary(testrestrictedmod)
#Like the original model and original restricted model, where lcavol, lweight, and svi are significant at the 95% level, here in the restricted test set lcavol, lweight, and svi are all again significant at the 95% level. Also, the Adjusted R-squared for the restricted test set model is 0.612.
set.seed(388)
prostateGLM <- glm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate, family = "gaussian")
set.seed(388)
cv.prostateGLM <- cv.glm(data=prostate, glmfit = prostateGLM, K = 3)
cv.prostateGLM$delta #Here, I add $delta in the cv.glm object to to get the delta values from the models, providing the cross-validated error rates from the model (usually the mean squared error). The cv.prostateGLM$delta gives me two values: The first value is the raw cross-validation estimate of the prediction error (mean squared error). The second value is the bias-corrected estimate, which accounts for the bias in the raw estimate. Using these will evaluate the modelâ€™s performance. Including this in my report will allow me to comment on how well my model generalizes to unseen data. https://www.rdocumentation.org/packages/boot/versions/1.3-30/topics/cv.glm
set.seed(388)
loocv.prostateGLM <- cv.glm(data=prostate, glmfit = prostateGLM, K = nrow(prostate))
#loocv.prostateGLM
loocv.prostateGLM$delta
modelSignif.05 <- glm(formula = lpsa ~ lcavol + lweight + svi, data = prostate, family = "gaussian")
cv.modelSignif.05 <- cv.glm(data=prostate, glmfit = modelSignif.05, K = 3)
cv.modelSignif.05$delta
install.packages("corrr")
rm(list=ls())
install.packages("dplyr")
library(dplyr)
install.packages("magrittr")
library(tidyverse)
library(magrittr)
library(ggplot2)
install.packages("texreg")
library(texreg)
library(stargazer)
library(performance)
install.packages("magrittr")
install.packages("dplyr")
#Once I have downloaded the newest version of R, the following ggplots will run:
#R Coding NOTE: ggplot histogram allows for more fine grain breaks than geom-bar. Also, consider using density plot over histogram but it can be preference.
#Cut:
ggplot(diamonds, aes(y = price, x = cut)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(x = color, fill=cut)) +
geom_bar(position = "dodge")+
theme_bw()
diamonds %>%
filter(carat <= 2) %>% #***Does this temporarily filter carat as =2 because I do not want to permanently change the variable?
ggplot(aes(x = carat, fill=cut)) +
geom_bar(position = "dodge") +
theme_bw()
# ***The code does not run.
#Color:
ggplot(diamonds, aes(y = price, x = color)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(x = clarity, fill=color)) +
geom_bar(position = "dodge")+
theme_bw()
#Clarity:
ggplot(diamonds, aes(y = price, x = clarity)) +
geom_jitter() +
theme_bw()
# talk about how this is surprising
#Carat:
ggplot(diamonds, aes(y = price, x = carat)) +
geom_jitter() +
theme_bw()
ggplot(diamonds, aes(y = price, x = carat)) +
geom_point() +
geom_smooth(method = "lm") +
theme_bw()
# ran this based on the strength of the relationship
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
library(tidyverse)
library(ggplot2)
expected_prior <- sum(c(0.1, .3, .6, .8, .95)*0.2)
prior <- rep(0.2, 5)
theta <- c(0.1, .3, .6, .8, .95)
likelihood <- dbinom(21, 68, theta)
posterior <- likelihood * 0.2 / (0.2*sum(likelihood))
print(posterior)
plot(y = prior,
x = theta,
ylim = c(0, 1),
title('Prior and Posterior Probability'),
ylab = 'Probability', xlab = expression(theta),
pch = 19, col = 'cornflowerblue')
points(x = theta, y = posterior, col = 'forestgreen', pch = 17)
legend('topright', legend=c("Prior", "Posterior"), pch=c(19, 17),
col=c("cornflowerblue", "forestgreen"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
beta_prime <- beta + (116 - 17) #Observed Failures
alpha <- 1
beta <- 4
alpha_prime <- alpha + 17 #Observed Successes
beta_prime <- beta + (116 - 17) #Observed Failures
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- alpha_prime*beta_prime / (((alpha_prime + beta_prime)^2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)^2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)(alpha_prime + beta_prime + 1))
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)*(alpha_prime + beta_prime + 1))
#From Expected Value and Variance for Beta Distribution
post_mean <- alpha_prime / (alpha_prime + beta_prime)
post_var <- (alpha_prime*beta_prime) / (((alpha_prime + beta_prime)**2)*(alpha_prime + beta_prime + 1))
cred_int_95 <- qbeta(c(.025, .975), alpha_prime,beta_prime)
#Beta(2,2)
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta(2,2)")
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 20, 20), add=T, lwd=2, col='cornflowerblue')
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 20, 20), add=T, lwd=2, col='cornflowerblue')
curve(dbeta(x, 2, 6), add=T, lwd=2, col='firebrick')
curve(dbeta(x, 6, 2), add=T, lwd=2, col='forestgreen')
alphas <- c(2, 20, 2, 6)
curve(dbeta(x, 1, 1.439))#,
curve(dbeta(x, 1, 1.439))
## All together
curve(dbeta(x, 2, 2), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta distributions", lwd=2, ylim=c(0, 5))
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 5),xlim = c(0,2), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 1, 1.439),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 2), col = 'forestgreen')
curve(dbeta(x, 2, 2.878), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(1, 2), ", ",  c(1.439, 2.878), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
1.439*4
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 2), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5,756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5,756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5.756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
#### FIX #######
curve(dbeta(x, 2, 2.878),
ylab=expression(pi(theta)), xlab=expression(theta),
main="Beta Distributions", lwd=2, ylim=c(0, 3), col = 'forestgreen')
curve(dbeta(x, 4, 5.756), add=T, lwd=2, col='cornflowerblue')
legend("topright",
legend=paste("(", c(2, 4), ", ",  c(2.878, 5.756), ")", sep=""),
lwd=2, col=c("forestgreen", "cornflowerblue"),
title=expression("("~alpha~","~beta~")"))
beta_var_1 <- (2*2.878) / (((2 + 2.878)**2)*(2 + 2.878 + 1))
beta_var_2 <- (4*5.756) / (((4 + 5.756)**2)*(4 + 5.756 + 1))
.04>.02
prob_3f <- qbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
#Beta(2, 6)
curve(dbeta(x, 2, 7), ylab=expression(pi(theta)), xlab=expression(theta), main="Beta(2,6)")
prob_3f <- dbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
prob_3f <- dbeta(0.1, shape1 = 2, shape2 = 7)
prob_3f <- pbeta(0.1, shape1 = 2, shape2 = 7)
prob_3f <- pbeta(0.1, shape1 = 2, shape2 = 7, lower.tail = FALSE)
quantile_3g <- qbeta(0.4, shape1 = 2, shape2 = 7, lower.tail = TRUE)
library(readr)
action_movies <- read_csv("Desktop/Fall 2024/Stat 386/GitHubRepos/lab-06-chale15/action_movies.csv")
View(action_movies)
cor(action_movies$Score, action_movies$Rank)
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(naivebayes)
library(ranger)
library(kknn)
library(discrim)
library(kernlab)
setwd("~/Desktop/Fall 2024/Stat 348/GitHubRepos/amazon-employee-access-challenge/")
train <- vroom("train.csv")
test <- vroom("test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=1)
svm_radial <- svm_rbf(rbf_sigma=tune(),
cost=tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_polynomial <- svm_poly(degree=tune(),
cost=tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_lin <- svm_linear(cost=tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
radial_workflow <- workflow() %>%
add_model(svm_radial) %>%
add_recipe(my_recipe)
poly_workflow <- workflow() %>%
add_model(svm_polynomial) %>%
add_recipe(my_recipe)
lin_workflow <- workflow() %>%
add_model(svm_lin) %>%
add_recipe(my_recipe)
tuning_grid_radial <- grid_regular(rbf_sigma(), cost(), levels = 5)
tuning_grid_poly <- grid_regular(degree(), cost(), levels = 5)
tuning_grid_lin <- grid_regular(cost(), levels = 5)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results_poly <- poly_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid_poly,
metrics = metric_set(roc_auc))
best_tune_poly <- cv_results_poly %>% select_best(metric='roc_auc')
final_workflow_poly <- poly_workflow %>%
finalize_workflow(best_tune_poly) %>%
fit(data = train)
poly_svm_preds <- predict(final_workflow_poly,
new_data = test,
type = 'prob')
poly_svm_submission <- poly_svm_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=poly_svm_submission, file="./Submissions/PolySVMpreds1.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(naivebayes)
library(ranger)
library(kknn)
library(discrim)
setwd("~/Desktop/Fall 2024/Stat 348/GitHubRepos/amazon-employee-access-challenge/")
train <- vroom("train.csv")
test <- vroom("test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=1) %>%
step_smote(all_outcomes(), neighbors=3)
install.packages('themis')
library(themis)
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=1) %>%
step_smote(all_outcomes(), neighbors=3)
log_reg_model <- logistic_reg() %>%
set_engine('glm')
log_reg_workflow <- workflow() %>%
add_model(log_reg_model) %>%
add_recipe(my_recipe) %>%
fit(data = train)
log_reg_preds <- predict(log_reg_workflow,
new_data = test,
type = 'prob')
log_reg_submission <- log_reg_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=log_reg_submission, file="./LogRegPreds5.csv", delim=",")
plog_model <- logistic_reg(mixture=tune(), penalty = tune()) %>%
set_engine('glmnet')
plog_workflow <- workflow() %>%
add_model(plog_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(penalty(), mixture(), levels = 100)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- plog_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
library(tidyverse)
library(tidymodels)
library(embed)
library(vroom)
library(workflows)
library(glmnet)
library(naivebayes)
library(ranger)
library(kknn)
library(discrim)
library(themis)
setwd("~/Desktop/Fall 2024/Stat 348/GitHubRepos/amazon-employee-access-challenge/")
train <- vroom("train.csv")
test <- vroom("test.csv")
train <- train %>% mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=1) %>%
step_smote(all_outcomes(), neighbors=3)
log_reg_model <- logistic_reg() %>%
set_engine('glm')
log_reg_workflow <- workflow() %>%
add_model(log_reg_model) %>%
add_recipe(my_recipe) %>%
fit(data = train)
log_reg_preds <- predict(log_reg_workflow,
new_data = test,
type = 'prob')
log_reg_submission <- log_reg_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=log_reg_submission, file="./LogRegPreds6.csv", delim=",")
my_recipe <- recipe(ACTION~., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_predictors(), threshold=0.9) %>%
step_smote(all_outcomes(), neighbors=3)
log_reg_model <- logistic_reg() %>%
set_engine('glm')
log_reg_workflow <- workflow() %>%
add_model(log_reg_model) %>%
add_recipe(my_recipe) %>%
fit(data = train)
log_reg_preds <- predict(log_reg_workflow,
new_data = test,
type = 'prob')
log_reg_submission <- log_reg_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=log_reg_submission, file="./Submissions/LogRegPreds7.csv", delim=",")
plog_model <- logistic_reg(mixture=tune(), penalty = tune()) %>%
set_engine('glmnet')
plog_workflow <- workflow() %>%
add_model(plog_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(penalty(), mixture(), levels = 100)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- plog_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- plog_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
plog_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
plog_submission <- plog_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=plog_submission, file="./Submissions/PLogPreds5.csv", delim=",")
knn_model <- nearest_neighbor(neighbors=tune()) %>%
set_mode("classification") %>%
set_engine("kknn")
knn_workflow <- workflow() %>%
add_model(knn_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(neighbors(), levels = 100)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- knn_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- knn_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
knn_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
knn_submission <- knn_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=knn_submission, file="./Submissions/KNNPreds5.csv", delim=",")
nb_model <- naive_Bayes(Laplace=tune(),
smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes")
nb_workflow <- workflow() %>%
add_model(nb_model) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(Laplace(), smoothness(), levels = 5)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- nb_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- nb_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
nb_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
nb_submission <- nb_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=nb_submission, file="./Submissions/NBPreds5.csv", delim=",")
balanced_rf <- rand_forest(mtry=tune(),
min_n=tune(),
trees=500) %>%
set_mode("classification") %>%
set_engine("ranger")
balanced_workflow <- workflow() %>%
add_model(balanced_rf) %>%
add_recipe(my_recipe)
tuning_grid <- grid_regular(mtry(range=c(1, 9)), min_n(), levels = 3)
folds <- vfold_cv(train, v = 10, repeats=1)
cv_results <- balanced_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>% select_best(metric='roc_auc')
final_workflow <- balanced_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = train)
balanced_preds <- predict(final_workflow,
new_data = test,
type = 'prob')
balanced_submission <- balanced_preds %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(ACTION = .pred_1)
vroom_write(x=balanced_submission, file="./Submissions/BalancedPredsRF1.csv", delim=",")
